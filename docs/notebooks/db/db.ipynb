{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of Database\n",
    "\n",
    "The `DB()` object in the `dl_utils.db` module encapsulates functionality to organize project data and track data transformations as well as analysis. As the size and complexity of a project grows, use of the `DB()` object will ensure that:\n",
    "\n",
    "1. Project file paths are defined in a well-organized directory hierarchy\n",
    "2. Project metadata is serialized in a clear and standard format\n",
    "3. Code to perform data transformations and analysis are well-documented with clear input(s) and output(s)\n",
    "4. The entire data transformation pipeline can be applied easily to new cohorts\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "* Initialization\n",
    "* Basic functionality\n",
    "* Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up\n",
    "\n",
    "The following lines of code prepare the requisite environment to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from dl_utils.db import DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition this tutorial assumes that the `bet` example dataset has been downloaded in the `dl_utils/data` folder. If needed, the following lines of code will download the required data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_utils import datasets\n",
    "\n",
    "# --- Set paths\n",
    "DAT_PATH = '../../../data'\n",
    "CSV_PATH = '{}/bet/csvs/db-all.csv.gz'.format(DAT_PATH)\n",
    "YML_PATH = '{}/bet/ymls/db-all.yml'.format(DAT_PATH)\n",
    "\n",
    "# --- Download data\n",
    "if not os.path.exists(CSV_PATH):\n",
    "    datasets.download(name='bet', path=DAT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "\n",
    "All `DB()` object data including filenames and raw values are stored in a `*.csv` (or `*.csv.gz`) file. If necessary, all `DB()` object metadata such as filename directory roots, filename patterns or method definitions are stored in a `*.yml` file. Either file type may be passed directly into the `DB(...)` constructor to create a new object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating from a `*.csv` file\n",
    "\n",
    "All underlying raw `DB()` data is stored in a `*.csv` file. Each row in the `*.csv` file represents a single exam. Each column in the `*.csv` file may be one of three different types: sid, fnames, header.\n",
    "\n",
    "An example template `*.csv` file is shown here:\n",
    "\n",
    "```\n",
    "sid           fname-dat       fname-lbl       hemorrhage\n",
    "exam-id-000   /000/dat.hdf5   /000/lbl.hdf5   True\n",
    "exam-id-001   /001/dat.hdf5   /001/lbl.hdf5   False\n",
    "exam-id-002   /002/dat.hdf5   /002/lbl.hdf5   True\n",
    "...           ...             ...             ...\n",
    "```\n",
    "\n",
    "#### sid (required)\n",
    "\n",
    "Exactly one column in the `*.csv` file must be named `sid` and be populated with a **unique** study ID for each exam (row). The `sid` may be either numeric or alphanumeric in content.\n",
    "\n",
    "#### fnames (optional)\n",
    "\n",
    "If a project utilizes one or more serialized data volumes, the file paths should be maintained in columns specified with a `fname-` prefix (e.g. `fname-dat` and `fname-lbl` as above). Files may be listed using either complete or relative paths, or using a number of keywords. See `*.yml` configuration below for more information.\n",
    "\n",
    "#### header (optional)\n",
    "\n",
    "All other data for a project should be maintained in the remaining columns of the `*.csv` file (e.g. not `sid` and not prefixed with `fname-`). It is best practice to serialize a single value per column (e.g. either a numeric value or string), rather than storing multiple values as an object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create from *.csv file\n",
    "db = DB(CSV_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating from a `*.yml` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the raw `DB()` data stored in a `*.csv` file, various metadata that defines `DB()` behavior may also be specified in a corresponding `*.yml` file. \n",
    "\n",
    "An example template `*.yml` file is shown here:\n",
    "\n",
    "```yml\n",
    "files: \n",
    "  csv: /csvs/db-all.csv.gz\n",
    "  yml: /ymls/db-all.yml\n",
    "paths: \n",
    "  data: /path/to/data\n",
    "  code: /path/to/code\n",
    "sform: {}\n",
    "query: {}\n",
    "fdefs: []\n",
    "\n",
    "```\n",
    "\n",
    "#### files and paths (required)\n",
    "\n",
    "To facilitate transfer of code and data, relative paths are stored in the `files` variable, with path roots stored in the `pqths` variable. Thus:\n",
    "\n",
    "```python\n",
    "paths['code'] + files['csv'] # complete path to *.csv file\n",
    "paths['code'] + files['yml'] # complete path to *.yml file\n",
    "```\n",
    "\n",
    "Additionally, `paths['data']` represents the root directory for serialized data volumes.\n",
    "\n",
    "#### sform (optional)\n",
    "\n",
    "There are a two different methods to store files paths in the `*.csv` table (in columns prefixed with `fname-`). As above, the simplest method is to use the complete file path name. Alternatively, the `sform` dictionary may be set with key-values pairs where the key represents a column name and the value represents a Python string format pattern. The Python string format pattern may use one of three different keywords:\n",
    "\n",
    "* *root*: the data root directory (`paths['data'` as above)\n",
    "* *curr*: the current contents stored in the `*.csv` file\n",
    "* *sid*: the current exam study ID\n",
    "\n",
    "Consider the following examples (using the same template `*.csv` as above):\n",
    "\n",
    "##### Example 1\n",
    "\n",
    "```yml\n",
    "sform:\n",
    "  dat: '{root}/{curr}'\n",
    "  lbl: '{root}/{curr}`\n",
    "```\n",
    "\n",
    "... would be expanded to ...\n",
    "\n",
    "```\n",
    "sid           fname-dat                    fname-lbl       \n",
    "exam-id-000   /path/to/data/000/dat.hdf5   /path/to/data/000/lbl.hdf5\n",
    "exam-id-001   /path/to/data/001/dat.hdf5   /path/to/data/001/lbl.hdf5\n",
    "exam-id-002   /path/to/data/002/dat.hdf5   /path/to/data/002/lbl.hdf5\n",
    "...           ...             ...             ...\n",
    "```\n",
    "\n",
    "##### Example 2\n",
    "\n",
    "```yml\n",
    "sform:\n",
    "  dat: '{root}/{sid}/dat.hdf5'\n",
    "  lbl: '{root}/{sid}/lbl.hdf5`\n",
    "```\n",
    "\n",
    "... would be expanded to ...\n",
    "\n",
    "```\n",
    "sid           fname-dat                            fname-lbl       \n",
    "exam-id-000   /path/to/data/exam-id-000/dat.hdf5   /path/to/data/exam-id-000/lbl.hdf5\n",
    "exam-id-001   /path/to/data/exam-id-001/dat.hdf5   /path/to/data/exam-id-001/lbl.hdf5\n",
    "exam-id-002   /path/to/data/exam-id-002/dat.hdf5   /path/to/data/exam-id-002/lbl.hdf5\n",
    "...           ...             ...             ...\n",
    "```\n",
    "\n",
    "#### query (optional)\n",
    "\n",
    "As an alternative to manually identifying the relevant file paths, a simple query can configured to automatically find (and update) the requested data. The query dictinoary is defined simply using a root data directory and one or more matching suffix patterns. \n",
    "\n",
    "Consider the following example:\n",
    "\n",
    "```yml\n",
    "query:\n",
    "  root: /path/to/data\n",
    "  dat: dat.hdf5\n",
    "  lbl: lbl.hdf5\n",
    "```\n",
    "\n",
    "In this scenario:\n",
    "\n",
    "* column `fname-dat`: populated with results from `glob.glob('/path/to/data/**/dat.hdf5')`\n",
    "* column `fname-lbl`: populated with results from `glob.glob('/path/to/data/**/lbl.hdf5')`\n",
    "\n",
    "Note that corresponding `dat.hdf5` and `lbl.hdf5` files for the same exam are expected to be in the **same subdirectory**.\n",
    "\n",
    "#### fdefs (optional)\n",
    "\n",
    "See notes below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create from *.yml file\n",
    "db = DB(YML_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon creating a `DB()` object, the underlying data structure is split into two separate `pandas` DataFrames, `db.fnames` and `db.header` (each DataFrame is an attribute of the main `DB()` object). The `db.fnames` DataFrame comprises of all `*.csv` columns prefixed with `fname-` (with the `fname-` prefix itself removed upon import); the `db.header` DataFrame contains all remaining columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Inspect fnames and header\n",
    "assert type(db.header) is pd.DataFrame\n",
    "assert type(db.fnames) is pd.DataFrame\n",
    "\n",
    "# --- Ensure all five exams are available\n",
    "assert db.fnames.shape[0] == 5\n",
    "assert db.header.shape[0] == 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## db.fnames\n",
    "\n",
    "The `db.fnames` attribute is implemented via a Pandas DataFrame. The DataFrame index and columns represent the individual exam study IDs (`sid`) and different filenames (`fname-`), respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- db.fnames\n",
    "db.fnames.index\n",
    "db.fnames.columns\n",
    "\n",
    "# --- db.fnames shape (row x columns)\n",
    "db.fnames.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some useful functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Check to see if fnames exist (all columns)\n",
    "db.exists()\n",
    "\n",
    "# --- Check to see if fnames exist (specified column)\n",
    "db.exists(cols=['dat'])\n",
    "\n",
    "# --- Return fully expanded fnames of a single row (specified by sid)\n",
    "fnames = db.fnames_expand_single(sid='ID_2e28736ab7')\n",
    "\n",
    "# --- Return fully expanded fnames of a single row (specified by index)\n",
    "fnames = db.fnames_expand_single(index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## db.header\n",
    "\n",
    "The `db.header` attribute is implemented via a Pandas DataFrame. The DataFrame index and columns represent the individual exam study IDs (`sid`) and different exam metadata, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- db.fnames\n",
    "db.header.index\n",
    "db.header.columns\n",
    "\n",
    "# --- db.fnames shape (row x columns)\n",
    "db.header.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that by the index (and number of rows) for `db.fnames` and `db.header` must always be indentical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (db.fnames.index == db.header.index).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To return an entire combined row of data (fully expanded filenames and header metadata), use the `db.row(...)` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Return complete row of data (specified by sid)\n",
    "row = db.row(sid='ID_2e28736ab7')\n",
    "\n",
    "# --- Return complete row of data (specified by index)\n",
    "row = db.row(index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration\n",
    "\n",
    "Perhaps the most common task in data science involves iterating through a dataset and applying a number of different functions or transformations to the cohort. Accordingly, the `DB()` object is optimized specifically for this type of workflow pattern.\n",
    "\n",
    "The most simple method to manually iterate through a dataset is to use the `db.cursor(...)` Python generator. In it's most simple use case, the underlying `db.fnames` and `db.header` DataFrames are accessed efficiently using the `itertuples()` method, with fnames expanded based on string format patterns (`db.sform`) as needed. By default, a progress bar is printed (although this may be turned off with the `verbose=False` flag)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sid, fnames, header in db.cursor():\n",
    "    # --- Perform data transformation and analysis here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate through partial dataset\n",
    "\n",
    "As needed either a binary mask or an array of indices may be provided to iterate through only a portion of the dataset. A common use case is to keep track of some useful marker in `db.header` (e.g. `cohort-positive` for all exams with a positive finding), and to use that header column as a mask for iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Iterate through every exam with a sid ending in '7'\n",
    "mask = [i[-1] == '7' for i in db.fnames.index]\n",
    "for sid, fnames, header in db.cursor(mask=mask):\n",
    "    pass\n",
    "\n",
    "# --- Iterate through the 0, 2 and 4 exams\n",
    "indices = [0, 2, 4]\n",
    "for sid, fnames, header in db.cursor(indices=indices):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate using multiple processes\n",
    "\n",
    "Given the highly parallel nature of iteration, oftentimes it may be useful to split up the task into separate processes. Given the single-threaded nature of the Python interpreter, the easiest way to implement parallel processing is to run multiple jobs in separate Python processes. \n",
    "\n",
    "The `DB()` facilitates this process by allowing the user to separate up the underlying data into `n` evenly-sized splits (specified by the `splits` argument). For any given individual Python process, the current data split can be identified by either by setting a shell environment variable `DL_SPLIT` prior to running the Python process, or by manually passing an additional `split` argument; note the former method is the recommended approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1: set current split using shell environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Iterate through split 1 of 2 (in file named data.py)\n",
    "for sid, fnames, header in db.cursor(splits=2):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, from the command line:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```$ DL_SPLIT=0 python data.py```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2: set current split using Python variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Iterate through split 1 of 2\n",
    "for sid, fnames, header in db.cursor(split=0, splits=2):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Functions\n",
    "\n",
    "When iterating through a dataset, the most common initial processing step is to load one or more serialized data files. In addition, upon completion of data transformation and/or analysis, the most common final processing step is to either serialize new data volumes and/or store intermediate results. To provide a uniform template for these operations, and to facilitate an organized, explicitly documented approach to this complex process, the `DB()` object supports a high-level framework for performing these tasks.\n",
    "\n",
    "The general approach involves the following steps:\n",
    "\n",
    "* identify column(s) to use as function input(s)\n",
    "* identify column(s) to use as function output(s)\n",
    "* define function in separate Python file\n",
    "\n",
    "Here, any arbitrary function that performs some combination of data transformation, processing and/or analysis across the dataset may be considered. Once these key attributes have been determined, the function definition and parameters are added to the `fdefs` list variable in the database `*.yml` file. \n",
    "\n",
    "For illustration purposes, let us consider the following `*.yml` file:\n",
    "\n",
    "```yml\n",
    "sform:\n",
    "  - kwargs:\n",
    "      arr: dat-org\n",
    "    lambda: null\n",
    "    python: \n",
    "      file: /defs/xform/data.py\n",
    "      name: create_samp\n",
    "    return:\n",
    "      arr: dat-256\n",
    "```\n",
    "\n",
    "Let us also consider the following `/defs/xform/data.py` file:\n",
    "\n",
    "```python\n",
    "def create_samp(arr):\n",
    "    \n",
    "    return {'arr': arr[::2, ::2, ::2]}\n",
    "```\n",
    "\n",
    "Using these examples, let us examine the following step-by-step process for defining the custom function.\n",
    "\n",
    "1. Identify the input argument(s) of the function in `kwargs`\n",
    "\n",
    "The input argument(s) to the function are defined in the `kwargs` dict within the `sform` variable. Each key-value pair is coded such that the key represents the keyword-delimited argument name in the custom function signature, and the value represents the source of populated data. Note that values can be derived from the following sources:\n",
    "\n",
    "* column in `db.fnames`: the value will be replaced with the loaded data\n",
    "* column in `db.header`: the value will be passed directly into the function\n",
    "* constant: if the value does not match any column, then it will be passed as a constant\n",
    "\n",
    "In the above example, if there is a column in `db.fnames` named `dat-org`, then the corresponding file will be loaded and passed as `arr` into the custom function. \n",
    "\n",
    "2. Identify the output argument(s) of the function in `return`\n",
    "\n",
    "The return of all custom functions must be a Python dictionary. For any dictionary item that is to be serialized (as a file) or stored (in a column within `db.header`), an appropriate mapping must be made in the `return` dict within the `sform` variable. Note that as before, the values of the dict can reference one of the following sources:\n",
    "\n",
    "* column in `db.fnames`: the data will be serialized at the specified file path\n",
    "* column in `db.header`: the data will be stored in corresponding `db.header` column\n",
    "\n",
    "In the abovoe example, if there is a column in `db.fnames` named `dat-256`, then the corresponding returned array in `{'arr': ...}` will be serialized at the file path in the `db.fnames['dat-256']` column.\n",
    "\n",
    "3. Define the function in either `lambda` or `python`\n",
    "\n",
    "Most commonly, the function code itself will be implemented in a separate Python file. The location of the file is defined by the `paths['code']` directory root as well as the `python['file']` dict in the `sform` variable. The method name of the corresponding function within the Python file is defined in the `python['name']` variable.\n",
    "\n",
    "### Running custom functions\n",
    "\n",
    "To run a custom function, simply use the `db.refresh(...)` method to update all row contents. The following arguments should be passed:\n",
    "\n",
    "* **cols**: a list of column name(s) to update; based on these column names, all matching functions (e.g. those whose return(s) match the specified names) will be executed\n",
    "* **load**: the method used to load files (e.g. `io.load`); this method may be custom function depending on underlying file format; if no function is passed, all matching file names will simply be passed as strings\n",
    "\n",
    "See examples below:\n",
    "\n",
    "```python\n",
    "from dl_utils import io\n",
    "\n",
    "# --- Run function that generates dat-256 output\n",
    "db.refresh(cols=['dat-256'], load=io.load)\n",
    "```\n",
    "\n",
    "Note that the `db.refresh(...)` method accepts all of the kwargs described above in `db.cursor(...)` to iterate using multiple processes, masks, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
